# Day 40

> 2021-10-14

## 参考资料

- https://zhuanlan.zhihu.com/p/334658432
-

## 同步接口的缺点

例如，一个 ftp 服务器，当接收到客户机上传的文件，然后将文件写入到本机的过程中，若 ftp 服务程序忙于等待文件读写结果的返回，则会拒绝其他此刻正需要连接的客户机请求。显然，在这种场景下，更好的方式是采用异步编程模型，如在上述例子中，当服务器接收到某个客户机上传文件后，直接、无阻塞地将写入 IO 的 buffer 提交给内核，然后 caller 继续接受下一个客户请求，内核处理完 IO 之后，主动调用某种通知机制，告诉 caller 该 IO 已完成，完成状态保存在某位置，请查看。

## AIO

### 优点

当大量读事件堆积到 IO 设备的队列中时，将会发挥出内核中“电梯算法”的优势，从而降低随机读取磁盘扇区的成本。

### 缺点

- 不支持缓存操作

即使需要操作的文件块在 linux 文件缓存中存在，也不会通过操作缓存中的文件块来代替实际对磁盘的操作。目前，nginx 仅支持在读取文件时使用 AIO，因为正常写入文件往往是写入内存就立刻返回，效率很高，如果替换成 AIO 写入速度会明显下降。

- 仅支持 direct IO

在采用 AIO 的时候，只能使用 O_DIRECT，不能借助文件系统缓存来缓存当前的 IO 请求，还存在 size 对齐（直接操作磁盘，所有写入内存块数量必须是文件系统块大小的倍数，而且要与内存页大小对齐。）等限制，直接影响了 aio 在很多场景的使用。

- 拷贝开销大

每个 IO 提交需要拷贝 64+8 字节，每个 IO 完成需要拷贝 32 字节，总共 104 字节的拷贝。这个拷贝开销是否可以承受，和单次 IO 大小有关：如果需要发送的 IO 本身就很大，相较之下，这点消耗可以忽略，而在大量小 IO 的场景下，这样的拷贝影响比较大。

- API 不友好

每一个 IO 至少需要两次系统调用才能完成（submit 和 wait-for-completion)，需要非常小心地使用完成事件以避免丢事件。

- 系统调用开销大

也正是因为上一条，io_submit/io_getevents 造成了较大的系统调用开销，在存在 spectre/meltdown（CPU 熔断幽灵漏洞，CVE-2017-5754）的机器上，若如果要避免漏洞问题，系统调用性能则会大幅下降。在存储场景下，高频系统调用的性能影响较大。

## io_uring

### 共享 ring buffer 的好处

- 提交、完成请求时节省应用和内核之间的内存拷贝
- 使用 SQPOLL 高级特性时，应用程序无需调用系统调用
- 无锁操作，用 memory ordering 实现同步，通过几个简单的头尾指针的移动就可以实现快速交互

### io 流程

#### io 准备

```cpp
/*
 * Sets up an aio uring context, and returns the fd. Applications asks for a
 * ring size, we return the actual sq/cq ring sizes (among other things) in the
 * params structure passed in.
 */
static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
{
    struct io_uring_params p;
    int i;

    if (copy_from_user(&p, params, sizeof(p)))
        return -EFAULT;
    for (i = 0; i < ARRAY_SIZE(p.resv); i++) {
        if (p.resv[i])
            return -EINVAL;
    }

    if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
            IORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |
            IORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |
            IORING_SETUP_R_DISABLED))
        return -EINVAL;

    return  io_uring_create(entries, &p, params);
}
```

经过标志位非法检查之后，关键是调用内部函数 io_uring_create 实现实例创建过程。

```cpp
static int io_uring_create(unsigned entriesstatic int io_uring_create(unsigned entries, struct io_uring_params *p,
               struct io_uring_params __user *params)
{
    struct user_struct *user = NULL;
    struct io_ring_ctx *ctx;
    bool limit_mem;
    int ret;

    if (!entries)
        return -EINVAL;
    if (entries > IORING_MAX_ENTRIES) {
        if (!(p->flags & IORING_SETUP_CLAMP))
            return -EINVAL;
        entries = IORING_MAX_ENTRIES;
    }

    /*
     * Use twice as many entries for the CQ ring. It's possible for the
     * application to drive a higher depth than the size of the SQ ring,
     * since the sqes are only used at submission time. This allows for
     * some flexibility in overcommitting a bit. If the application has
     * set IORING_SETUP_CQSIZE, it will have passed in the desired number
     * of CQ ring entries manually.
     */
    p->sq_entries = roundup_pow_of_two(entries);
    if (p->flags & IORING_SETUP_CQSIZE) {
        /*
         * If IORING_SETUP_CQSIZE is set, we do the same roundup
         * to a power-of-two, if it isn't already. We do NOT impose
         * any cq vs sq ring sizing.
         */
        if (!p->cq_entries)
            return -EINVAL;
        if (p->cq_entries > IORING_MAX_CQ_ENTRIES) {
            if (!(p->flags & IORING_SETUP_CLAMP))
                return -EINVAL;
            p->cq_entries = IORING_MAX_CQ_ENTRIES;
        }
        p->cq_entries = roundup_pow_of_two(p->cq_entries);
        if (p->cq_entries < p->sq_entries)
            return -EINVAL;
    } else {
        p->cq_entries = 2 * p->sq_entries;
    }

    user = get_uid(current_user());
    limit_mem = !capable(CAP_IPC_LOCK);

    if (limit_mem) {
        ret = __io_account_mem(user,
                ring_pages(p->sq_entries, p->cq_entries));
        if (ret) {
            free_uid(user);
            return ret;
        }
    }

    ctx = io_ring_ctx_alloc(p);
    if (!ctx) {
        if (limit_mem)
            __io_unaccount_mem(user, ring_pages(p->sq_entries,
                                p->cq_entries));
        free_uid(user);
        return -ENOMEM;
    }
    ctx->compat = in_compat_syscall();
    ctx->user = user;
    ctx->creds = get_current_cred();
#ifdef CONFIG_AUDIT
    ctx->loginuid = current->loginuid;
    ctx->sessionid = current->sessionid;
#endif
    ctx->sqo_task = get_task_struct(current);

    /*
     * This is just grabbed for accounting purposes. When a process exits,
     * the mm is exited and dropped before the files, hence we need to hang
     * on to this mm purely for the purposes of being able to unaccount
     * memory (locked/pinned vm). It's not used for anything else.
     */
    mmgrab(current->mm);
    ctx->mm_account = current->mm;

#ifdef CONFIG_BLK_CGROUP
    /*
     * The sq thread will belong to the original cgroup it was inited in.
     * If the cgroup goes offline (e.g. disabling the io controller), then
     * issued bios will be associated with the closest cgroup later in the
     * block layer.
     */
    rcu_read_lock();
    ctx->sqo_blkcg_css = blkcg_css();
    ret = css_tryget_online(ctx->sqo_blkcg_css);
    rcu_read_unlock();
    if (!ret) {
        /* don't init against a dying cgroup, have the user try again */
        ctx->sqo_blkcg_css = NULL;
        ret = -ENODEV;
        goto err;
    }
#endif

    /*
     * Account memory _before_ installing the file descriptor. Once
     * the descriptor is installed, it can get closed at any time. Also
     * do this before hitting the general error path, as ring freeing
     * will un-account as well.
     */
    io_account_mem(ctx, ring_pages(p->sq_entries, p->cq_entries),
               ACCT_LOCKED);
    ctx->limit_mem = limit_mem;

    ret = io_allocate_scq_urings(ctx, p);
    if (ret)
        goto err;

    ret = io_sq_offload_create(ctx, p);
    if (ret)
        goto err;

    if (!(p->flags & IORING_SETUP_R_DISABLED))
        io_sq_offload_start(ctx);

    memset(&p->sq_off, 0, sizeof(p->sq_off));
    p->sq_off.head = offsetof(struct io_rings, sq.head);
    p->sq_off.tail = offsetof(struct io_rings, sq.tail);
    p->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);
    p->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);
    p->sq_off.flags = offsetof(struct io_rings, sq_flags);
    p->sq_off.dropped = offsetof(struct io_rings, sq_dropped);
    p->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;

    memset(&p->cq_off, 0, sizeof(p->cq_off));
    p->cq_off.head = offsetof(struct io_rings, cq.head);
    p->cq_off.tail = offsetof(struct io_rings, cq.tail);
    p->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);
    p->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);
    p->cq_off.overflow = offsetof(struct io_rings, cq_overflow);
    p->cq_off.cqes = offsetof(struct io_rings, cqes);
    p->cq_off.flags = offsetof(struct io_rings, cq_flags);

    p->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |
            IORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |
            IORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |
            IORING_FEAT_POLL_32BITS;

    if (copy_to_user(params, p, sizeof(*p))) {
        ret = -EFAULT;
        goto err;
    }

    /*
     * Install ring fd as the very last thing, so we don't risk someone
     * having closed it before we finish setup
     */
    ret = io_uring_get_fd(ctx);
    if (ret < 0)
        goto err;

    trace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);
    return ret;
err:
    io_ring_ctx_wait_and_kill(ctx);
    return ret;
}
```

首先需要创建一个上下文结构 io_ring_ctx 用来管理整个会话。随后实现 SQ 和 CQ 内存区的映射，使用 IORING_OFF_CQ_RING 偏移量，使用 io_cqring_offsets 结构的实例，即 io_uring_params 中 cq_off 这个成员，SQ 使用 IORING_OFF_SQES 这个偏移量。其余的是一些错误检查、权限检查、资源配额检查等检查逻辑。

liburing 中使用 io_uring_setup 的部分代码:

```cpp
/*
 * Returns -1 on error, or zero on success. On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_init(unsigned entries, struct io_uring *ring, unsigned flags)
{
    struct io_uring_params p;
    int fd, ret;

    memset(&p, 0, sizeof(p));
    p.flags = flags;

    fd = io_uring_setup(entries, &p);
    if (fd < 0)
        return fd;

    ret = io_uring_queue_mmap(fd, &p, ring);
    if (ret)
        close(fd);

    return ret;
}
```

#### io 提交与收割

```cpp
int io_uring_enter(unsigned int fd, unsigned int to_submit,
                   unsigned int min_complete, unsigned int flags,
                   sigset_t *sig);
```

如果在 io_uring_setup 的时候 flag 设置了 IORING_SETUP_SQPOLL，内核会额外启动一个特定的内核线程来执行轮询的操作，称作 SQ 线程，这里使用的轮询结构会最终对应到 struct file_operations 中的 iopoll 操作。这里 io \_uring 实际上只有 vfs 层的改动，其它的都是使用已经存在的东西，而且几个核心的东西和 aio 使用的相同/类似。直接通过访问相关的队列就可以获取到执行完的任务，不需要经过系统调用。关于这个线程，通过 io_uring_params 结构中的 sq_thread_cpu 配置，这个内核线程可以运行在某个指定的 CPU 核心 上。这个内核线程会不停的 Poll SQ，直到在通过 sq_thread_idle 配置的时间内没有 Poll 到任何请求为止。

如果使用了 IORING_SETUP_SQPOLL 参数，IO 收割也不需要系统调用的参与。由于内核和用户态共享内存，所以收割的时候，用户态遍历 [cring->head, cring->tail) 区间，即已经完成的 IO 队列，然后找到相应的 CQE 并进行处理，最后移动 head 指针到 tail，IO 收割至此而终。

io_uring_enter 通过正确设置 IORING_ENTER_GETEVENTS，IORING_SETUP_IOPOLL 等 flag（如下代码设置 IORING_SETUP_IOPOLL 并且不设置 IORING_SETUP_SQPOLL，即没有使用 SQ 线程）调用 io_iopoll_check。

## 思考

- 性能瓶颈，iopoll 模式下，用户程序大部分使用在轮询，是处理请求的 sq_thread 不够多，还是磁盘性能不足
- 能否使用 nvm
